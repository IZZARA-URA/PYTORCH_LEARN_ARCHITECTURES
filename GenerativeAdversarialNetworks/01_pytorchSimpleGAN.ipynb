{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ee5195c9",
   "metadata": {},
   "source": [
    "# GAN https://www.youtube.com/watch?v=OXWvrRLzEaU&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=23\n",
    "\n",
    "Style Transfer \n",
    "Upscal images => DLSS\n",
    "\n",
    " DATA Augmentation \n",
    " Medical Application => privacy \n",
    " Semi-supervised Learning => SGAN\n",
    " \n",
    "    Generator : Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c122bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_34988/1509590845.py\", line 5, in <cell line: 5>\n",
      "    import torchvision\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/__init__.py\", line 6, in <module>\n",
      "    from torchvision import models\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/__init__.py\", line 12, in <module>\n",
      "    from . import detection\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
      "    from .faster_rcnn import *\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/detection/faster_rcnn.py\", line 4, in <module>\n",
      "    from torchvision.ops import MultiScaleRoIAlign\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/ops/__init__.py\", line 14, in <module>\n",
      "    _register_custom_op()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/ops/_register_onnx_ops.py\", line 104, in _register_custom_op\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/onnx/__init__.py\", line 284, in register_custom_op_symbolic\n",
      "    from torch.onnx import utils\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/onnx/utils.py\", line 1082, in register_custom_op_symbolic\n",
      "    \"it with the right domain and version.\".format(ns, op_name))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/onnx/symbolic_registry.py\", line 15, in <module>\n",
      "    from torch.onnx.symbolic_helper import _onnx_stable_opsets, _onnx_main_opset\n",
      "ImportError: cannot import name '_onnx_main_opset' from 'torch.onnx.symbolic_helper' (/opt/conda/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as f \n",
    "import torchvision\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Discriminator(nn.Module): \n",
    "    def __init__(self, img_dim): \n",
    "        super().__init__() \n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module): \n",
    "    def __init__(self, z_dim, img_dim): \n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "    \n",
    "# Hyperparameters etc. \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64 \n",
    "image_dim = 28 * 28 * 1\n",
    "batch_size = 32 \n",
    "num_epochs = 50 \n",
    "\n",
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5), )])\n",
    "\n",
    "dataset = datasets.MNIST(root=\"../dataset/\", transform=transforms, download=False)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "### torch.nn.BCELoss \n",
    "# L(x,y) = {Li, ... , Ln}^T\n",
    "# Ln = -wn[yn(log(xn)) + (1 -yn)log(1-xn)]\n",
    "# Mean(L(xx,y))\n",
    "# sum(L(xx,y))\n",
    "\n",
    "writer_fake = SummaryWriter(f\"runs/GAN_MINIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/GAN_MINIST/real\")\n",
    "step = 0 \n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    for batch_idx, (real, _) in enumerate(loader): \n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "        \n",
    "        ### Traning Discriminator : max log(D(real)) + log(1- D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2 \n",
    "        disc.zero_grad() \n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        ### Train Generator min log(1 - D(G(z))) <-> max log(D(G(z)))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        lossG.backward() \n",
    "        opt_gen.step() \n",
    "        \n",
    "        if batch_idx == 0: \n",
    "            print(f\" Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} Loss D: {lossD:.8f}, Loss G : {lossG:.8f}\")\n",
    "            \n",
    "            with torch.no_grad(): \n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize = True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize = True)\n",
    "                \n",
    "                writer_fake.add_image(\"MNIST Fake Images\", img_grid_fake, global_step=step)\n",
    "                writer_fake.add_image(\"MNIST Real Images\", img_grid_real, global_step=step)\n",
    "                \n",
    "                step += 1\n",
    "\n",
    "#         https://www.youtube.com/watch?v=OljTVUVzPpM&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=24    \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Things to try : \n",
    "    1. What happens if you use larger network? \n",
    "    2. Better normalization with BatchNorm \n",
    "    3. Different learning rate \n",
    "    4. Change architenture to CNN\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00417f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = real.view(-1, 784).to(device)\n",
    "batch_size = real.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f97516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2db309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064d80fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.fx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mf\u001b[39;00m \n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m \n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/convnext.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/ops/__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeform_conv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deform_conv2d, DeformConv2d\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiou_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance_box_iou_loss\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrop_block\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drop_block2d, drop_block3d, DropBlock2d, DropBlock3d\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_pyramid_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeaturePyramidNetwork\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfocal_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sigmoid_focal_loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/ops/drop_block.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.fx'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as f \n",
    "import torchvision\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Discriminator(nn.Module): \n",
    "    def __init__(self, img_dim): \n",
    "        super().__init__() \n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module): \n",
    "    def __init__(self, z_dim, img_dim): \n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "    \n",
    "# Hyperparameters etc. \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64 \n",
    "image_dim = 28 * 28 * 1\n",
    "batch_size = 32 \n",
    "num_epochs = 50 \n",
    "\n",
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5), )])     # Composes several transforms together.\n",
    "\n",
    "dataset = datasets.MNIST(root=\"../dataset/\", transform=transforms, download=False)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "### torch.nn.BCELoss \n",
    "# L(x,y) = {Li, ... , Ln}^T\n",
    "# Ln = -wn[yn(log(xn)) + (1 -yn)log(1-xn)]\n",
    "# Mean(L(xx,y))\n",
    "# sum(L(xx,y))\n",
    "\n",
    "writer_fake = SummaryWriter(f\"runs/GAN_MINIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/GAN_MINIST/real\")\n",
    "step = 0 \n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    for batch_idx, (real, _) in enumerate(loader): \n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        \n",
    "        ### Traning Discriminator : max log(D(real)) + log(1- D(G(z)))\n",
    "        disc_real = disc(real).view(-1)\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "            \n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        lossD_fake = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "        \n",
    "        lossD = (lossD_real + lossD_fake) / 2 \n",
    "        disc.zero_grad() \n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        ### Train Generator min log(1 - D(G(z))) <-> max log(D(G(z)))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step() \n",
    "        \n",
    "        if batch_idx == 0: \n",
    "            print(f\" Epoch [{epoch}/{num_epochs}] Loss D: {lossD:.8f}, Loss G : {lossG:.8f}\")\n",
    "            \n",
    "            with torch.no_grad(): \n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize = True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize = True)\n",
    "                \n",
    "                writer_fake.add_image(\"MNIST Fake Images\", img_grid_fake, global_step=step)\n",
    "                writer_real.add_image(\"MNIST Real Images\", img_grid_real, global_step=step)\n",
    "                \n",
    "                step += 1\n",
    "\n",
    "#         https://www.youtube.com/watch?v=OljTVUVzPpM&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=24    \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Things to try : \n",
    "    1. What happens if you use larger network? \n",
    "    2. Better normalization with BatchNorm \n",
    "    3. Different learning rate \n",
    "    4. Change architenture to CNN\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e48b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |███                             | 75.2 MB 109 kB/s eta 1:55:13"
     ]
    }
   ],
   "source": [
    "pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c53e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
